{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from Modules import BottleLinear as Linear\n",
    "from Modules import ScaledDotProductAttention,LayerNormalization\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import Constants\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sublayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,n_head,d_model,d_k,d_v,dropout=0.1):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.w_qs = nn.Parameter(torch.FloatTensor(n_head,d_model,d_k))\n",
    "        self.w_ks = nn.Parameter(torch.FloatTensor(n_head,d_model,d_k))\n",
    "        self.w_vs = nn.Parameter(torch.FloatTensor(n_head,d_model,d_v))\n",
    "        \n",
    "        self.attention = ScaleDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization(d_model)\n",
    "        self.proj = Linear(n_head*d_v,d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        init.xavier_normal(self.w_qs)\n",
    "        init.xavier_normal(self.w_ks)\n",
    "        init.xavier_normal(self.w_vs)\n",
    "        \n",
    "    def forward(self,q,k,v,attn_mask=None):\n",
    "        d_k,d_v = self.d_k,self.d_v\n",
    "        n_head = self.n_head\n",
    "        \n",
    "        residual = q\n",
    "        \n",
    "        mb_size, len_q, d_model = q.size()\n",
    "        mb_size, len_k, d_model = k.size()\n",
    "        mb_size, len_v, d_model = v.size()\n",
    "        \n",
    "        q_s = q.repeat(n_head,1,1).view(n_head,-1,d_model)\n",
    "        k_s = k.repeat(n_head,1,1).view(n_head,-1,d_model)\n",
    "        v_s = v.repeat(n_head,1,1).view(n_head,-1,d_model)\n",
    "        \n",
    "        q_s = torch.bmm(q_s,self.w_qs).view(-1,len_q,d_k)\n",
    "        k_s = torch.bmm(k_s,self.w_ks).view(-1,len_k,d_k)\n",
    "        v_s = torch.bmm(v_s,self.w_vs).view(-1,len_v,d_v)\n",
    "        \n",
    "        outputs,attns = self.attention(q_s,k_s,v_s,att_mask=attn_mask.repeat(n_head,1,1))\n",
    "        \n",
    "        outputs = torch.cat(torch.split(outputs,mb_size,dim=0),dim=-1)\n",
    "        outputs = self.proj(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        return self.layer_norm(outputs + residual),attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_hid,d_inner_hid,dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.w_1 = nn.Conv1d(d_hid,d_inner,1)\n",
    "        self.w_2 = nn.Conv1d(d_inner,d_hid,1)\n",
    "        self.layer_norm = LayerNormalization(d_hid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        outputs = self.relu(self.w_1(x.transpose(1,2)))\n",
    "        outputs = self.w_2(outputs).transpose(2,1)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return self.layer_norm(output+residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SubLayers import MultiHeadAttention,PositionwiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_inner_hid,n_head,d_k,d_v,dropout=0.1):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head,d_model,d_k,d_v,dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model,d_inner_hid,dropout=dropout)\n",
    "        \n",
    "    def forward(self,enc_input,self_attn_mask=None):\n",
    "        enc_output,enc_slf_attn = self.slf_attn(\n",
    "            enc_input,enc_input,enc_input,attn_mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output,enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_inner_hid,n_head,d_k,d_v,dropout=0.1):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head,d_model,d_k,d_v,dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head,d_model,d_k,d_v,dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model,d_inner_hid,dropout=dropout)\n",
    "        \n",
    "    def forward(self,dec_input,enc_output,slf_attn_mask=None,dec_enc_attn_mask=None):\n",
    "        dec_output,dec_slf_attn = self.slf_attn(\n",
    "            dec_input,dec_input,dec_input,attn_mask=slf_attn_mask)\n",
    "        dec_output,dec_enc_attn = self.enc_attn(\n",
    "            dec_output,enc_output,enc_output,attn_mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output,dec_slf_attn,dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Layers import EncoderLayer,DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position,d_pos_vec):\n",
    "    position_enc = nn.array([\n",
    "        [pos / np.power(10000,2*(j//2)/d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)\n",
    "    ])\n",
    "    position_enc[1:,0::2] = np.sin(position_enc[1:,0::2])\n",
    "    position_enc[1:,1::2] = np.cos(position_enc[1:,1::2])\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attn_padding_mask(seq_q,seq_k):\n",
    "    assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
    "    mb_size,len_q = seq_q.size()\n",
    "    mb_size,len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(Constants.PAD).unsqueeze(1)\n",
    "    pad_attn_mask = pad_attn_mask.expand(mb_size,len_q,len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    assert seq.dim() == 2\n",
    "    attn_shape = (seq.size(0),seq.size(1),seq.size(1))\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = np.concatenate((np.random.random((10,3)),np.zeros((3,3))),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "test_data = Variable(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(4 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(5 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(6 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(7 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(8 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(9 ,.,.) = \n",
       "  0  0  0\n",
       "  0  0  0\n",
       "  0  0  0\n",
       "\n",
       "(10,.,.) = \n",
       "  1  1  1\n",
       "  1  1  1\n",
       "  1  1  1\n",
       "\n",
       "(11,.,.) = \n",
       "  1  1  1\n",
       "  1  1  1\n",
       "  1  1  1\n",
       "\n",
       "(12,.,.) = \n",
       "  1  1  1\n",
       "  1  1  1\n",
       "  1  1  1\n",
       "[torch.ByteTensor of size 13x3x3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attn_padding_mask(test_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(4 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(5 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(6 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(7 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(8 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(9 ,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(10,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(11,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "\n",
       "(12,.,.) = \n",
       "  0  1  1\n",
       "  0  0  1\n",
       "  0  0  0\n",
       "[torch.ByteTensor of size 13x3x3]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attn_subsequent_mask(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data2 = torch.FloatTensor(np.random.random((13,3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.3283  0.2818  0.1895\n",
       "  0.8391  0.9610  0.6921\n",
       "  0.8609  0.7171  0.9459\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.0230  0.6590  0.4776\n",
       "  0.7684  0.3062  0.8785\n",
       "  0.3999  0.7001  0.4939\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.6848  0.0378  0.0957\n",
       "  0.9445  0.5699  0.9185\n",
       "  0.4937  0.5345  0.8517\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0.8327  0.2719  0.1924\n",
       "  0.2072  0.3360  0.3084\n",
       "  0.6014  0.9138  0.4349\n",
       "\n",
       "(4 ,.,.) = \n",
       "  0.0465  0.2373  0.9371\n",
       "  0.3756  0.4465  0.1945\n",
       "  0.2218  0.7685  0.5120\n",
       "\n",
       "(5 ,.,.) = \n",
       "  0.1172  0.6778  0.7712\n",
       "  0.0508  0.2874  0.6622\n",
       "  0.6880  0.0408  0.1986\n",
       "\n",
       "(6 ,.,.) = \n",
       "  0.2820  0.4468  0.9389\n",
       "  0.3088  0.8669  0.7539\n",
       "  0.2834  0.9564  0.4440\n",
       "\n",
       "(7 ,.,.) = \n",
       "  0.1776  0.1531  0.9330\n",
       "  0.0240  0.9338  0.4544\n",
       "  0.5160  0.1940  0.1774\n",
       "\n",
       "(8 ,.,.) = \n",
       "  0.0095  0.9395  0.8145\n",
       "  0.6572  0.3415  0.8222\n",
       "  0.3853  0.3665  0.0445\n",
       "\n",
       "(9 ,.,.) = \n",
       "  0.6685  0.2727  0.2630\n",
       "  0.4169  0.2593  0.9923\n",
       "  0.4314  0.0356  0.3390\n",
       "\n",
       "(10,.,.) = \n",
       "  0.0848  0.0101  0.2678\n",
       "  0.9473  0.6298  0.0969\n",
       "  0.3271  0.4015  0.6972\n",
       "\n",
       "(11,.,.) = \n",
       "  0.7665  0.4391  0.3739\n",
       "  0.3057  0.6941  0.0366\n",
       "  0.7976  0.5631  0.5600\n",
       "\n",
       "(12,.,.) = \n",
       "  0.1766  0.0293  0.4068\n",
       "  0.2693  0.1453  0.3916\n",
       "  0.5722  0.3722  0.5844\n",
       "[torch.FloatTensor of size 13x3x3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.3283  0.2818  0.1895\n",
       "  0.8391  0.9610  0.6921\n",
       "  0.8609  0.7171  0.9459\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.0230  0.6590  0.4776\n",
       "  0.7684  0.3062  0.8785\n",
       "  0.3999  0.7001  0.4939\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.6848  0.0378  0.0957\n",
       "  0.9445  0.5699  0.9185\n",
       "  0.4937  0.5345  0.8517\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0.8327  0.2719  0.1924\n",
       "  0.2072  0.3360  0.3084\n",
       "  0.6014  0.9138  0.4349\n",
       "\n",
       "(4 ,.,.) = \n",
       "  0.0465  0.2373  0.9371\n",
       "  0.3756  0.4465  0.1945\n",
       "  0.2218  0.7685  0.5120\n",
       "\n",
       "(5 ,.,.) = \n",
       "  0.1172  0.6778  0.7712\n",
       "  0.0508  0.2874  0.6622\n",
       "  0.6880  0.0408  0.1986\n",
       "\n",
       "(6 ,.,.) = \n",
       "  0.2820  0.4468  0.9389\n",
       "  0.3088  0.8669  0.7539\n",
       "  0.2834  0.9564  0.4440\n",
       "\n",
       "(7 ,.,.) = \n",
       "  0.1776  0.1531  0.9330\n",
       "  0.0240  0.9338  0.4544\n",
       "  0.5160  0.1940  0.1774\n",
       "\n",
       "(8 ,.,.) = \n",
       "  0.0095  0.9395  0.8145\n",
       "  0.6572  0.3415  0.8222\n",
       "  0.3853  0.3665  0.0445\n",
       "\n",
       "(9 ,.,.) = \n",
       "  0.6685  0.2727  0.2630\n",
       "  0.4169  0.2593  0.9923\n",
       "  0.4314  0.0356  0.3390\n",
       "\n",
       "(10,.,.) = \n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "\n",
       "(11,.,.) = \n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "\n",
       "(12,.,.) = \n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "[torch.FloatTensor of size 13x3x3]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2.masked_fill_(get_attn_padding_mask(test_data,test_data),-float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.3283    -inf    -inf\n",
       "  0.8391  0.9610    -inf\n",
       "  0.8609  0.7171  0.9459\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.0230    -inf    -inf\n",
       "  0.7684  0.3062    -inf\n",
       "  0.3999  0.7001  0.4939\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.6848    -inf    -inf\n",
       "  0.9445  0.5699    -inf\n",
       "  0.4937  0.5345  0.8517\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0.8327    -inf    -inf\n",
       "  0.2072  0.3360    -inf\n",
       "  0.6014  0.9138  0.4349\n",
       "\n",
       "(4 ,.,.) = \n",
       "  0.0465    -inf    -inf\n",
       "  0.3756  0.4465    -inf\n",
       "  0.2218  0.7685  0.5120\n",
       "\n",
       "(5 ,.,.) = \n",
       "  0.1172    -inf    -inf\n",
       "  0.0508  0.2874    -inf\n",
       "  0.6880  0.0408  0.1986\n",
       "\n",
       "(6 ,.,.) = \n",
       "  0.2820    -inf    -inf\n",
       "  0.3088  0.8669    -inf\n",
       "  0.2834  0.9564  0.4440\n",
       "\n",
       "(7 ,.,.) = \n",
       "  0.1776    -inf    -inf\n",
       "  0.0240  0.9338    -inf\n",
       "  0.5160  0.1940  0.1774\n",
       "\n",
       "(8 ,.,.) = \n",
       "  0.0095    -inf    -inf\n",
       "  0.6572  0.3415    -inf\n",
       "  0.3853  0.3665  0.0445\n",
       "\n",
       "(9 ,.,.) = \n",
       "  0.6685    -inf    -inf\n",
       "  0.4169  0.2593    -inf\n",
       "  0.4314  0.0356  0.3390\n",
       "\n",
       "(10,.,.) = \n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "\n",
       "(11,.,.) = \n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "\n",
       "(12,.,.) = \n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "    -inf    -inf    -inf\n",
       "[torch.FloatTensor of size 13x3x3]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2.masked_fill_(get_attn_subsequent_mask(test_data),-float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2  0  0\n",
       " 2  0  0  1  2\n",
       "[torch.LongTensor of size 2x5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([[0,1,2,0,0],[2,0,0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eye = torch.eye(10).long().unsqueeze(2).repeat(1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = torch.arange(0,10).long().unsqueeze(0).expand(10,-1).unsqueeze(2).repeat(1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear = nn.Linear(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(4 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(5 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(6 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(7 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(8 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "  0.4785 -0.3143  0.2136\n",
       "\n",
       "(9 ,.,.) = \n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.4785 -0.3143  0.2136\n",
       "  0.3405  0.4525  0.0260\n",
       "[torch.FloatTensor of size 10x10x3]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(Variable(eye).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_idx = torch.FloatTensor(256,38,38).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       "   0\n",
       "   1\n",
       "   2\n",
       " ⋮  \n",
       "  35\n",
       "  36\n",
       "  37\n",
       "\n",
       "( 1 ,.,.) = \n",
       "   0\n",
       "   1\n",
       "   2\n",
       " ⋮  \n",
       "  35\n",
       "  36\n",
       "  37\n",
       "\n",
       "( 2 ,.,.) = \n",
       "   0\n",
       "   1\n",
       "   2\n",
       " ⋮  \n",
       "  35\n",
       "  36\n",
       "  37\n",
       "... \n",
       "\n",
       "(125,.,.) = \n",
       "   0\n",
       "   1\n",
       "   2\n",
       " ⋮  \n",
       "  35\n",
       "  36\n",
       "  37\n",
       "\n",
       "(126,.,.) = \n",
       "   0\n",
       "   1\n",
       "   2\n",
       " ⋮  \n",
       "  35\n",
       "  36\n",
       "  37\n",
       "\n",
       "(127,.,.) = \n",
       "   0\n",
       "   1\n",
       "   2\n",
       " ⋮  \n",
       "  35\n",
       "  36\n",
       "  37\n",
       "[torch.LongTensor of size 128x38x1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,38).long().unsqueeze(0).expand(128,-1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       "   1   0   0  ...    0   0   0\n",
       "   0   1   0  ...    0   0   0\n",
       "   0   0   1  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    1   0   0\n",
       "   0   0   0  ...    0   1   0\n",
       "   0   0   0  ...    0   0   1\n",
       "\n",
       "( 1 ,.,.) = \n",
       "   1   0   0  ...    0   0   0\n",
       "   0   1   0  ...    0   0   0\n",
       "   0   0   1  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    1   0   0\n",
       "   0   0   0  ...    0   1   0\n",
       "   0   0   0  ...    0   0   1\n",
       "\n",
       "( 2 ,.,.) = \n",
       "   1   0   0  ...    0   0   0\n",
       "   0   1   0  ...    0   0   0\n",
       "   0   0   1  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    1   0   0\n",
       "   0   0   0  ...    0   1   0\n",
       "   0   0   0  ...    0   0   1\n",
       "... \n",
       "\n",
       "(253,.,.) = \n",
       "   1   0   0  ...    0   0   0\n",
       "   0   1   0  ...    0   0   0\n",
       "   0   0   1  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    1   0   0\n",
       "   0   0   0  ...    0   1   0\n",
       "   0   0   0  ...    0   0   1\n",
       "\n",
       "(254,.,.) = \n",
       "   1   0   0  ...    0   0   0\n",
       "   0   1   0  ...    0   0   0\n",
       "   0   0   1  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    1   0   0\n",
       "   0   0   0  ...    0   1   0\n",
       "   0   0   0  ...    0   0   1\n",
       "\n",
       "(255,.,.) = \n",
       "   1   0   0  ...    0   0   0\n",
       "   0   1   0  ...    0   0   0\n",
       "   0   0   1  ...    0   0   0\n",
       "     ...       ⋱       ...    \n",
       "   0   0   0  ...    1   0   0\n",
       "   0   0   0  ...    0   1   0\n",
       "   0   0   0  ...    0   0   1\n",
       "[torch.FloatTensor of size 256x38x38]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_idx.scatter_(2,torch.arange(0,38).long().unsqueeze(0).expand(256,-1).unsqueeze(2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n_max_seq,d_model,n_layers=6,n_heads=8,d_k=64,d_v=64,\n",
    "                 d_pos=512,d_inner_hid=2,dropout=0.1):\n",
    "        super(Encoder,self).__init__()\n",
    "        n_position = n_max_seq + 1\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,d_pos,padding_idx=Constants.PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(\n",
    "            n_position,d_pos)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model,d_inner_hid,n_head,d_k,d_v,dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "    def forward(self,src_seq,src_pos,return_attns=False):\n",
    "        enc_input = src_seq\n",
    "        enc_input += self.position_enc(src_pos)\n",
    "        if return_attns:\n",
    "            enc_slf_attns = []\n",
    "        enc_output = enc_input\n",
    "        enc_slf_attn_mask = get_attn_padding_mask(\n",
    "            src_seq[:,:,0],src_seq[:,:,0])\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slt_attn = enc_layer(\n",
    "                enc_output,slf_attn_mask=enc_slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attns += [enc_slf_attn]\n",
    "        if return_attns:\n",
    "            return enc_output,enc_slf_attns\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_max_seq,d_model,n_layers=6,n_head=8,d_k=64,d_v=64,\n",
    "                d_pos=512,d_inner_hid=1024,dropout=0.1):\n",
    "        super(Decoder,self).__init__()\n",
    "        n_position = n_max_seq + 1\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,d_pos,padding_idx=Constants.PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(\n",
    "            n_position,d_pos)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model,d_inner_hid,n_head,d_k,d_v,dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "    def forward(self,tgt_seq,tgt_pos,src_seq,enc_output,return_attns=False):\n",
    "        dec_input = tgt_seq\n",
    "        dec_input += self.position_enc(tgt_pos)\n",
    "        dec_slf_attn_mask = get_attn_padding_mask(\n",
    "            tgt_seq[:,:,0],tgt_seq[:,:,0])\n",
    "        dec_enc_attn_pad_mask = get_attn_padding_mask(\n",
    "            tgt_seq[:,:,0],src_seq[:,:,0])\n",
    "        if return_attns:\n",
    "            dec_slf_attns,dec_enc_attns = [],[]\n",
    "        dec_output = dec_input\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output,dec_slf_attn,dec_enc_attn = dec_layer(\n",
    "                dec_output,enc_output,\n",
    "                slf_attn_mask=dec_slf_attn_mask,\n",
    "                dec_enc_attn_mask=dec_enc_attn_pad_mask)\n",
    "            if return_attns:\n",
    "                dec_slf_attns += [dec_slf_attn]\n",
    "                dec_enc_attns += [dec_enc_attn]\n",
    "        if return_attns:\n",
    "            return dec_output,dec_slf_attns,dec_enc_attns\n",
    "        return dec_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
